import numpy as np
import math
import matplotlib.pyplot as plt
import sys
import time

# # for test
formatted_train_input = "largeoutput/model2_formatted_train.tsv"
formatted_valid_input = 'largeoutput/model2_formatted_valid.tsv'
formatted_test_input = 'largeoutput/model2_formatted_test.tsv'
dict_input = 'dict.txt'
# output
train_output = 'train_out_test.labels'
test_output = 'test_out_test.labels'
metrics_out = 'metrics_out_test.txt'
num_epoch = 200

#
# # for argv
# formatted_train_input = sys.argv[1]
# formatted_valid_input = sys.argv[2]
# formatted_test_input = sys.argv[3]
# dict_input = sys.argv[4]
# # output
# train_output = sys.argv[5]
# test_output = sys.argv[6]
# metrics_out = sys.argv[7]
# num_epoch = int(sys.argv[8])

# constant pram
learn_rate = 0.1


plot_Y_train =[]
plot_Y_valid =[]

def convertDict(fileName):
    dict = {}
    file = open(fileName)
    for l in file:
        temp = l.strip().split(" ")
        dict[temp[0]] = temp[1]
    return dict


def convertXY(fileName):
    file = open(fileName)
    X = []
    Y = []
    for line in file:
        tmp = line[1:].strip().split('\t')
        t = []

        for i in tmp:
            tmp2 = i.split(":")
            t.append(int(tmp2[0]))
        t.append(-1)
        X.append(t)
        Y.append(int(line[0]))
    print("Y:", Y)
    print("X:", X)
    return X, Y


class LR:
    def __init__(self, X, Y, dict):
        # pram
        self.X = X
        self.Y = Y
        self.N = len(Y)  # length of data set
        self.M = len(dict)  # number of features
        self.theta = np.zeros((1, self.M + 1))  # the BOSS
        # self.thetaT = np.zeros((self.M, 1))

    def train(self):
        print("--------- Trainning Started ----------")
        for i in range(num_epoch):
            print("start:", i)
            self.SGD()

            j_test = self.J(X_train, Y_train)
            j_valid = self.J(X_valid, Y_valid)

            plot_Y_train.append(j_test/len(Y_train))
            plot_Y_valid.append(j_valid/len(Y_valid))

            print("theta:", self.theta)
        print("--------- Trainning Finished ----------")

    def predict(self, predict_X, test_Y):
        p = self.calP(predict_X)
        er = self.test(test_Y, p)
        print("--------- Predict Finished ----------")
        return p, er

    def test(self, test_Y, p):
        error = 0
        for i in range(len(test_Y)):
            if test_Y[i] != p[i]:
                error += 1
        er = error / len(test_Y)
        print("error rate:", er)
        return er

    # loop all SGD units for theta
    def exp(self, xi):
        thetaTx = 0.0
        for x in xi:
            a = self.theta[0][x]
            thetaTx += a
        # print(self.thetaTx)
        exp = math.exp(thetaTx)
        # print(exp)
        return exp

    def SGD(self):
        for i in range(self.N):
            xi = self.X[i]
            # print(len(xi))
            exp = self.exp(xi)
            for j in range(len(xi)):
                self.SGD_unit(i, j, xi, exp)

    # unit SGD for one ThetaJ
    def SGD_unit(self, i, j, xi, exp):
        partb = self.Y[i] - (exp / (1 + exp))
        parta = learn_rate * partb
        self.theta[0][xi[j]] += parta

    def calP(self, X):
        P = np.zeros(len(X))
        for i in range(len(X)):
            xi = X[i]
            exp = self.exp(xi)
            p = exp / (exp + 1)
            if p >= 0.5:
                p = 1
            else:
                p = 0
            P[i] = p
        return P

    def J(self,X ,Y):
        j = 0
        for i in range(len(X)):
            xi = X[i]
            exp, thetaTx= self.exp2(xi)
            parta = (-Y[i])*thetaTx+np.log(1+exp)
            j+=parta
        return  j

    def exp2(self, xi):
        thetaTx = 0.0
        for x in xi:
            a = self.theta[0][x]
            thetaTx += a
        # print(self.thetaTx)
        exp = math.exp(thetaTx)
        # print(exp)
        return exp, thetaTx

    # def NCLog(self):
#     for i in range(self.N):
#         xi = self.X[i]
#         parta = (-self.Y[i])*(np.dot(self.theta.transpose(), self.X[i]))
#         exp = np.exp(np.dot(self.theta.transpose(), xi))
#         partb = np.log(1+exp)
#         combine = parta+partb
#         self.J.append(combine)
#     print(self.J)



def writeMet(er_train, er_test, met_out):
    f = open(met_out, 'w')
    f.write("error(train): " + str(er_train) + "\n")
    f.write("error(test): " + str(er_test))


def writeLab(p, lab_out):
    f = open(lab_out, 'w')
    for i in range(len(p)):
        temp = str(p[i]).split(".")
        if i == len(p):
            f.write(temp[0])
        else:
            f.write(temp[0] + "\n")


def plot(a,b):
    Xs = []
    for i in range(200):
        Xs.append(i)
    plt.plot(Xs, a)
    plt.plot(Xs, b)
    plt.interactive(False)
    plt.title("Model1")
    plt.xlabel("Number of Epoch")
    plt.ylabel("J")
    plt.show()

if __name__ == '__main__':
    # read in the X and Y
    start_time = time.time()

    global X_train, Y_train, X_test, Y_test, X_valid, Y_valid

    X_train, Y_train = convertXY(formatted_train_input)
    X_test, Y_test = convertXY(formatted_test_input)
    X_valid, Y_valid = convertXY(formatted_valid_input)
    # read in the dict
    dict = convertDict(dict_input)
    dict_len = len(dict)
    # creat a new LR
    lr = LR(X_train, Y_train, dict)
    # train
    lr.train()
    # test
    p_train, er_train = lr.predict(X_train, Y_train)
    p_test, er_test = lr.predict(X_test, Y_test)
    p_valid, er_valid = lr.predict(X_valid, Y_valid)

    print(p_train[0])

    writeLab(p_train, train_output)
    writeLab(p_test, test_output)
    writeMet(er_train, er_test, metrics_out)

    print(plot_Y_train)
    print(plot_Y_valid)

    # plot_Y_train = [1418.1017575551634, 1333.4492912222306, 1308.5567765301942, 1175.4625539867152, 1096.7314307496197, 1078.5830346177213, 1046.864859111139, 866.1242589264666, 789.7938394591428, 677.0987027104455, 625.1534523749515, 534.6003006998656, 450.41622061835585, 391.41567785219013, 338.8897870643563, 291.05108527353906, 248.03933844000866, 209.34184882988572, 175.0763161036537, 145.00685748895236, 118.7179704823403, 95.8868521091223, 76.28301840869383, 59.71464627990833, 45.97667893019871, 34.82323625374859, 25.96957197021832, 19.104737862521898, 13.906755155936512, 10.062551247129946, 7.284057643025528, 5.3157341878866164, 3.9401059837316748, 2.983028738347429, 2.314127809752895, 1.8411188423645406, 1.5010610536151194, 1.2518408777073102, 1.0654372683150168, 0.9231414408827959, 0.8123476355658413, 0.7244548592637159, 0.653508523674723, 0.5953213578827967, 0.5469014064065272, 0.5060764922518332, 0.4712448927877735, 0.441207720767464, 0.4150547263418181, 0.39208542711620054, 0.37175388909377344, 0.3536295432982894, 0.33736901520382784, 0.3226956144013514, 0.309384219559872, 0.2972500098925696, 0.2861399713161121, 0.27592642685267516, 0.26650205983056835, 0.25777604938075427, 0.24967104290248354, 0.2421207642477205, 0.23506810908281778, 0.22846361676803417, 0.2222642355820034, 0.21643231824519915, 0.21093479956544914, 0.20574251910596358, 0.2008296600948488, 0.1961732820922456, 0.19175292973164457, 0.18755030353828098, 0.18354898167806485, 0.17973418370761526, 0.17609256913308985, 0.1726120649522925, 0.16928171743917478, 0.1660915642921377, 0.1630325239584613, 0.16009629950382614, 0.15727529484528374, 0.15456254153119248, 0.1519516345504225, 0.14943667589628895, 0.14701222481344786, 0.14467325382033658, 0.14241510973896904, 0.14023347907777775, 0.13812435720940563, 0.13608402086552937, 0.13410900353847977, 0.1321960734364544, 0.13034221368729942, 0.12854460452672792, 0.12680060724219072, 0.12510774967255456, 0.1234637130903902, 0.12186632031413809, 0.12031352491791235, 0.11880340142121894, 0.11733413635629199, 0.11590402012238896, 0.11451143954653784, 0.11315487108039703, 0.11183287456994451, 0.1105440875425587, 0.1092872199614543, 0.10806104940346285, 0.10686441662035111, 0.10569622144882285, 0.1045554190368529, 0.10344101635845879, 0.10235206899130797, 0.10128767813388419, 0.10024698784195549, 0.09922918246543286, 0.09823348426871344, 0.09725915121940046, 0.0963054749315977, 0.09537177875101739, 0.09445741597084638, 0.09356176816767006, 0.09268424364847574, 0.09182427599967148, 0.0909813227307128, 0.09015486400497147, 0.08934440145139352, 0.08854945705079599, 0.08776957209174843, 0.08700430619058201, 0.0862532363710731, 0.08551595619972865, 0.08479207497248682, 0.0840812169495471, 0.08338302063484253, 0.0826971380972351, 0.08202323433039192, 0.08136098664923676, 0.0807100841199518, 0.08007022702187595, 0.07944112633894401, 0.07882250327875025, 0.07821408881776351, 0.07761562327071593, 0.07702685588279055, 0.07644754444337976, 0.07587745491970097, 0.07531636110950268, 0.0747640443114295, 0.0742202930119017, 0.07368490258812395, 0.07315767502536893, 0.07263841864859404, 0.07212694786692919, 0.07162308293067872, 0.07112664969994052, 0.07063747942425712, 0.07015540853273144, 0.06968027843385004, 0.06921193532479758, 0.06875023000935997, 0.0682950177243536, 0.06784615797371668, 0.0674035143703149, 0.06696695448451541, 0.06653634969978696, 0.06611157507437176, 0.06569250920921112, 0.0652790341213636, 0.0648710351231416, 0.06446840070611642, 0.06407102243023034, 0.06367879481749217, 0.06329161525009858, 0.06290938387281883, 0.06253200349931029, 0.06215937952237339, 0.06179141982773455, 0.061428034711323416, 0.06106913680003036, 0.06071464097512886, 0.060364464299365705, 0.06001852594639518, 0.05967674713319925, 0.05933905105498906, 0.05900536282267961, 0.05867560940283623, 0.05834971955955451, 0.05802762379901089, 0.05770925431562994, 0.057394544940582964, 0.057083431091913925, 0.05677584972669106, 0.05647173929485943, 0.056171039694668114, 0.055873692229795914, 0.0555796395679535, 0.05528882570098785, 0.05500119590634309, 0.054716696709988716]
    # plot_Y_valid = [216.9930702599591, 209.69782486790805, 203.04100371818186, 189.15137432266133, 174.41474184536716, 178.91623845845274, 182.6420118797729, 156.43079842128884, 151.14100173416787, 139.5837586278614, 132.46332427132614, 121.55131277451487, 109.50938158774841, 101.70046186716415, 95.05925789943306, 89.01300552546611, 83.70082701986912, 78.8871235153036, 74.53846142557057, 70.55396276897565, 66.82339975412998, 63.30787745658242, 60.018032116830604, 56.96966223117532, 54.16252858221044, 51.58379585168836, 49.21862271561635, 47.05603601482061, 45.08943324273035, 43.31483463902711, 41.72875829337829, 40.32639881456621, 39.10042784405421, 38.04053509450357, 37.13363393296401, 36.36455086563875, 35.716987749668284, 35.17453943848949, 34.721573955750905, 34.343852896100366, 34.02886236435336, 33.765897869774875, 33.54597958592404, 33.36167340652167, 33.20687522444361, 33.07659482898909, 32.96675873348645, 32.874039791863744, 32.795714805603474, 32.729547977579045, 32.67369672941239, 32.626636147291045, 32.587098591233826, 32.55402547557853, 32.526528739320895, 32.50385999783713, 32.485385774018056, 32.47056754211069, 32.45894558737788, 32.4501258986111, 32.443769478632, 32.439583589360794, 32.437314550569816, 32.43674179139879, 32.43767291611624, 32.43993959440392, 32.44339412469002, 32.44790654912507, 32.453362222511615, 32.459659756270455, 32.466709273440806, 32.474430922603354, 32.48275360813528, 32.49161390185295, 32.50095510726961, 32.51072645268618, 32.52088239339393, 32.531382006574354, 32.54218846519175, 32.55326857939736, 32.564592395795785, 32.57613284644166, 32.58786544068962, 32.5997679940672, 32.61182038921368, 32.624004364657644, 32.63630332782054, 32.648702189149134, 32.661187214716804, 32.673745895002, 32.68636682786604, 32.69903961401898, 32.711754763489196, 32.724503611807776, 32.73727824478434, 32.75007143089464, 32.76287656042348, 32.77568759061324, 32.788498996158054, 32.801305724466935, 32.81410315518455, 32.826887063520935, 32.83965358699233, 32.85239919522185, 32.86512066248824, 32.877815042746064, 32.890479646870894, 32.90311202191159, 32.915709932152794, 32.928271341814806, 32.940794399233866, 32.95327742238366, 32.96571888561331, 32.9781174074891, 32.9904717396391, 33.002780756510354, 33.01504344595632, 33.02725890058132, 33.0394263097757, 33.051544952380496, 33.06361418992881, 33.07563346041312, 33.087602272535214, 33.09952020039752, 33.11138687859887, 33.1232019977025, 33.13496530004419, 33.1466765758543, 33.158335659667436, 33.16994242699766, 33.18149679125769, 33.19299870090269, 33.20444813678143, 33.21584510967943, 33.22718965803727, 33.238481845833455, 33.24972176061732, 33.26090951168099, 33.27204522836092, 33.28312905845854, 33.29416116677152, 33.30514173372728, 33.31607095411195, 33.32694903588714, 33.33777619908827, 33.34855267479957, 33.35927870419893, 33.36995453766833, 33.38058043396595, 33.39115665945385, 33.401683487379465, 33.41216119720521, 33.42259007398468, 33.432970407780566, 33.44330249312312, 33.45358662850471, 33.4638231159096, 33.474012260375716, 33.484154369585674, 33.49424975348702, 33.5042987239377, 33.51430159437646, 33.524258679515526, 33.53417029505515, 33.54403675741774, 33.553858383500625, 33.563635490446174, 33.57336839542843, 33.58305741545464, 33.59270286718118, 33.602305066742574, 33.61186432959363, 33.62138097036232, 33.63085530271407, 33.640287639226486, 33.64967829127297, 33.65902756891594, 33.66833578080811, 33.67760323410156, 33.686830234364386, 33.69601708550414, 33.705164089697455, 33.714271547326284, 33.72333975691914, 33.73236901509811, 33.741359616530474, 33.7503118538853, 33.759226017794276, 33.76810239681622, 33.77694127740595, 33.78574294388655, 33.79450767842483, 33.80323576100972, 33.8119274694343, 33.82058307927965, 33.829202863902076, 33.83778709442195, 33.846336039715545, 33.85484996640798, 33.86332913886934]
    plot(plot_Y_train,plot_Y_valid)

    end_time = time.time()
    print("Running Time:", (end_time - start_time) / 60)
