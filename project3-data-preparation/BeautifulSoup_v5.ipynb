{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use BeautifulSoup to parse The New York Times and Fox News articles   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Web Pages and HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways of extracting or importing data from the Internet. As the tasks you just done, you can use APIs to retrieve information from any major website such as Twitter, Twitch, Instagram, Facebook which provides APIs to access their website dataset. And all this data available in a structured form.\n",
    "\n",
    "But there are some drawbacks of API Web Scraping. First, most of the website doesnâ€™t provide an API. Second, the results are usually in a somewhat raw form with no formatting or visual representation (like the results from a database query) so it is far from ideal for end users since it takes some cognitive overhead to interpret the raw information.\n",
    "\n",
    "Yet, if we have HTML it is quite easy for a human to visually interpret it, but to try to perform some type of programmatic analysis we first need to parse the HTML into a more structured form.\n",
    "\n",
    "As a general rule of thumb, if the data you need can be accessed or retrieved in a structured form (either from a bulk download or API) prefer that first. But if the data you want (and need) is not as in our case we need to resort to alternative (messier) means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse a Coronavirus Article from The New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `BeautifulSoup`, parse the HTML of an article about coronavirus from The New York Time to extract in a structured form. Fill in following function stubs to parse a single page of article and return: \n",
    "\n",
    "1. the article features as a structured Python dictionary\n",
    "2. the total number of words in the article (do not include the word counts of the summary, only the content!)\n",
    "\n",
    "Be sure to structure your Python dictionary as follows (to be graded correctly). The order of the keys doesn't matter, only the keys and the data type of the values matters:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Title': 'F.D.A. Approves First Coronavirus Antibody Test in U.S.' #str\n",
    "    'Author': 'Apoorva Mandavilli' # list, a list of author names\n",
    "    'Date': '2020-04-03' # str, yyyy-mm-dd\n",
    "    'Summary': '.....' #str, a paragraph summarize the article\n",
    "    'Content': '.....' #list, the whole article content, every element is a paragraph\n",
    "}\n",
    "```\n",
    "\n",
    "Note: Remember to remove blank lines or redundant part for every element. Some articles do not include a summary, deal with this problem and make Summary=\"No summary\" when there's no summary for the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from testing.testing import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve a url using BeautifulSoup\n",
    "def retrieve_url(url):\n",
    "    page =requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING parse_page_nyt: PASSED 16/16\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def article_check_nyt(article):\n",
    "    type_check = lambda field, typ: field in article and typ(article[field])\n",
    "    test.true(type_check(\"Title\", lambda r: isinstance(r, str)))\n",
    "    test.true(type_check(\"Author\", lambda r: isinstance(r, list)))\n",
    "\n",
    "    datecheck = re.compile(\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "    test.true(type_check(\"Date\", lambda r: datecheck.match(r)))\n",
    "    test.true(type_check(\"Summary\", lambda r: isinstance(r, str)))\n",
    "    test.true(type_check(\"Content\", lambda r: isinstance(r, list)))\n",
    "\n",
    "def parse_page_nyt_test(parse_page_nyt):\n",
    "    article1, num_words_1 = parse_page_nyt(\"https://www.nytimes.com/2020/04/21/health/fda-in-home-test-coronavirus.html?searchResultPosition=1\")\n",
    "    article_check_nyt(article1)\n",
    "    test.equal(article1['Title'], \"F.D.A. Authorizes First In-Home Test for Coronavirus\")\n",
    "    test.equal(len(article1['Summary']), 119)\n",
    "    test.equal(num_words_1, 3343)\n",
    "    article2, num_words_2 = parse_page_nyt(\"https://www.nytimes.com/2020/04/18/health/kidney-dialysis-coronavirus.html?searchResultPosition=9\")\n",
    "    article_check_nyt(article2)\n",
    "    test.equal(len(article2['Author']), 4)\n",
    "    test.equal(article2['Date'], '2020-04-18')\n",
    "    test.equal(num_words_2, 11249)\n",
    "\n",
    "@test\n",
    "def parse_page_nyt(url):\n",
    "\n",
    "    \"\"\"\n",
    "    Parse the article on a single page of The New York Times.\n",
    "    \n",
    "    Args:\n",
    "        html (string): String of HTML corresponding to a Coronavirus related article from The New York Times\n",
    "\n",
    "    Returns:\n",
    "        Tuple(Dict, int): a tuple of two elements\n",
    "            first element: The dictionary of this single article\n",
    "            second element: number of words in the content\n",
    "    \"\"\"\n",
    "    \n",
    "    soup=retrieve_url(url)\n",
    "    \n",
    "    dic={}\n",
    "    \n",
    "    dic['Title']=soup.find(\"title\").get_text().replace(\" - The New York Times\", \"\")\n",
    "    \n",
    "    dic['Author']=[]\n",
    "    for i in soup.find_all('span', itemprop=\"name\"):\n",
    "        dic['Author'].append(i.get_text())\n",
    "        \n",
    "    dic['Date']=soup.find(\"time\")[\"datetime\"][:10]\n",
    "    \n",
    "    if soup.find('p', id=\"article-summary\") is not None:\n",
    "        dic['Summary']=soup.find('p', id=\"article-summary\").get_text()\n",
    "    else:\n",
    "        dic['Summary']=\"No summary\"\n",
    "        \n",
    "    dic['Content']=[]\n",
    "    for i in soup.find_all('p', class_='css-exrw3m evys1bk0')[:-1]:\n",
    "        dic['Content'].append(i.get_text())\n",
    "        \n",
    "    \n",
    "    word_count=0\n",
    "    for paragraph in dic['Content']:\n",
    "        word_count+=len(paragraph)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dic, word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Several Coronavirus Article from The New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to parse a single page information from the New York Times using `BeautifulSoup`. However, sometimes we would like to parse several articles in a quick way. Parsing an article a time is not a time-efficient way. Let's start parsing several articles a time!\n",
    "\n",
    "\n",
    "\n",
    "In order to get the same data for everyone, let's set some conditions when getting articles from The New York Times. \n",
    "1. Search `coronavirus` in search box of The New York Times home page\n",
    "2. Set the Date range from 2020/3/27 to 4/27  * I would basically set a date range that around 200 articles are posted\n",
    "3. Set the section to `health`\n",
    "4. Set the type to `Article`\n",
    "\n",
    "For each article, use `parse_page_nyt` to get their dicitionaries and article word counts. Return two things:\n",
    "1. a list of tuple(include a dictionary and article word counts) in the order that they are present on the page\n",
    "2. the number of articles *Note: do not get the number of the articles from the search page, use len(list) to get it!\n",
    "\n",
    "In this function, we have to use `webdriver` in `selenium` package to handle the multi-pages problem. Please refer to the documentation of `selenium`: https://selenium-python.readthedocs.io/ to get more information on this package. \n",
    "\n",
    "Remember to state `time.sleep()` when parsing several pages using `selenium` since the website might interupt your visits if you entered the website too many times in a short time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the url by webdriver\n",
    "def retrieve_url_by_driver(url):\n",
    "    driver = webdriver.Chrome(\"./chromedriver\", options=options)\n",
    "    driver.get(url)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING parse_several_pages_nyt: PASSED 13/13\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_several_pages_nyt_test(parse_several_pages_nyt):\n",
    "    articles, num_articles = parse_several_pages_nyt(\"https://www.nytimes.com/search?dropmab=false&endDate=20200417&query=coronavirus&sections=Health%7Cnyt%3A%2F%2Fsection%2F9f943015-a899-5505-8730-6d30ed861520&sort=best&startDate=20200327&types=article\")\n",
    "    \n",
    "    article_10, num_wc_10=articles[10]\n",
    "    article_check_nyt(article_10)\n",
    "    test.equal(num_wc_10, 13178)\n",
    "    \n",
    "    article_38, num_wc_38=articles[38]\n",
    "    article_check_nyt(article_38)\n",
    "    test.equal(num_wc_38, 5315)\n",
    "    \n",
    "    test.equal(num_articles, 70)\n",
    "\n",
    "@test\n",
    "def parse_several_pages_nyt(base_url):\n",
    "    \"\"\"\n",
    "    Retrieve ALL of the articles(include their content) for a single page on The New York Times.\n",
    "\n",
    "    Args:\n",
    "        url (string): The New York Times URL of the searched page.\n",
    "\n",
    "    Returns:\n",
    "        Tuple(List(tuple), int): a tuple of two elements\n",
    "            first element: a list of tuple(include a dictionary and article word counts) of the articles in the searched page\n",
    "            second element: the number of articles\n",
    "    \"\"\"\n",
    "    \n",
    "    driver=retrieve_url_by_driver(base_url)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            driver.find_element_by_css_selector(\"button[data-testid='search-show-more-button']\").click()\n",
    "            time.sleep(3)\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    \n",
    "    ans=[]\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"a\", href=True):\n",
    "        if \"searchResultPosition\" in i['href']:\n",
    "            if \"=0\" not in i['href']:\n",
    "                ans.append(parse_page_nyt(\"https://www.nytimes.com\"+i['href']))\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return ans, len(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse a Coronavirus Article from Fox News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing several articles from The New York Times, let's parse some articles from other resources. This time, let's parse a article from Fox News. \n",
    "\n",
    "Similar to `parse_page_nyt`, `parse_page_fn` also returns two elements:\n",
    "1. the article features as a structured Python dictionary\n",
    "2. the total number of words in the article\n",
    "\n",
    "\n",
    "Be sure to structure your Python dictionary as follows (to be graded correctly). The order of the keys doesn't matter, only the keys and the data type of the values matters:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Title': 'F.D.A. Approves First Coronavirus Antibody Test in U.S.' #str\n",
    "    'Author': 'Apoorva Mandavilli' # list, a list of author names\n",
    "    'Date': '2020-04-03' # str, yyyy-mm-dd\n",
    "    'Content': '.....' #list, the whole article content, every element is a paragraph\n",
    "}\n",
    "```\n",
    "\n",
    "Note 1: Fox News do not include a summary but you have to capture the discriptions under pictures as a paragraph of the content.\n",
    "Note 2: If there's no date specified, set Date=\"No Date Specified\".\n",
    "Note 3: If author and article source are both provided, `Author` should only include the `Author`; if only the article source is provided, `Author` should be the article source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING parse_page_fn: PASSED 13/13\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def article_check_fn(article):\n",
    "    type_check = lambda field, typ: field in article and typ(article[field])\n",
    "    test.true(type_check(\"Title\", lambda r: isinstance(r, str)))\n",
    "    test.true(type_check(\"Author\", lambda r: isinstance(r, list)))\n",
    "\n",
    "    datecheck = re.compile(\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "    test.true(type_check(\"Date\", lambda r: datecheck.match(r)))\n",
    "    test.true(type_check(\"Content\", lambda r: isinstance(r, list)))\n",
    "\n",
    "def parse_page_fn_test(parse_page_fn):\n",
    "    article1, num_words_1 = parse_page_fn(\"https://www.foxnews.com/health/dying-alone-coronavirus-volunteers-ipads-virtual-connect\")\n",
    "    article_check_fn(article1)\n",
    "    test.equal(article1['Title'], \"Dying alone from coronavirus: Group collects used iPads to virtually connect patients with family\")\n",
    "    test.equal(num_words_1, 4574)\n",
    "    article2, num_words_2 = parse_page_fn(\"https://www.foxnews.com/health/is-it-safe-go-into-supermarkets-amid-coronavirus-outbreak\")\n",
    "    article_check_fn(article2)\n",
    "    test.equal(article2['Author'], ['David Aaro'])\n",
    "    test.equal(article2['Date'], '2020-04-06')\n",
    "    test.equal(num_words_2, 6467)\n",
    "\n",
    "@test\n",
    "def parse_page_fn(url):\n",
    "\n",
    "    \"\"\"\n",
    "    Parse the article on a single page of Fox News.\n",
    "    \n",
    "    Args:\n",
    "        html (string): String of HTML corresponding to a Coronavirus related article from Fox News\n",
    "\n",
    "    Returns:\n",
    "        Tuple(Dict, int): a tuple of two elements\n",
    "            first element: The dictionary of this single article\n",
    "            second element: number of words in the content\n",
    "    \"\"\"\n",
    "    \n",
    "    dic={}\n",
    "    \n",
    "    page =requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    if soup.find(\"h1\", class_='headline')!=None:\n",
    "        dic['Title']=soup.find(\"h1\", class_='headline').get_text()\n",
    "    else:\n",
    "        dic['Title']=soup.find(\"h1\", class_='title').get_text()\n",
    "\n",
    "    dic['Author']=[]\n",
    "    if soup.find(\"div\", class_='author-byline') is not None:\n",
    "        for i in soup.find(\"div\", class_='author-byline').find(\"span\"):\n",
    "            if \"By\" not in i:\n",
    "\n",
    "                if i.find(\"a\")!= None and type(i.find(\"a\"))!=int:\n",
    "                    dic['Author'].append(i.find(\"a\", href=True ).get_text())\n",
    "                    if \"|\" in dic['Author'][-1]:\n",
    "                        del dic['Author'][-1]\n",
    "                        dic['Author'].append(i.get_text().split(\"|\")[0].strip())\n",
    "                else:\n",
    "                    dic['Author'].append(i.get_text())\n",
    "    else:\n",
    "        dic['Author'].append(\"No author\")\n",
    "     \n",
    "    if soup.find(\"time\")is not None:\n",
    "        date_str=soup.find(\"time\").get_text().strip()+\", 2020\"\n",
    "        datetime_obj = datetime.datetime.strptime(date_str, '%B %d, %Y')   \n",
    "        dic['Date']=str(datetime_obj.date())\n",
    "    else:\n",
    "        dic['Date']=\"No Date Specified\"\n",
    "    \n",
    "        \n",
    "    dic['Content']=[]\n",
    "    \n",
    "    if len(soup.find_all('p'))>4:\n",
    "        for i in soup.find_all('p')[3:-4]:\n",
    "            if not i.find(\"strong\"):\n",
    "                dic['Content'].append(i.get_text().strip())\n",
    "    else:\n",
    "        for i in soup.find_all('p', itemprop=\"description\"):\n",
    "             dic['Content'].append(i.get_text().strip())\n",
    "        \n",
    "        \n",
    "    \n",
    "    word_count=0\n",
    "    for paragraph in dic['Content']:\n",
    "        word_count+=len(paragraph)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dic, word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Several Coronavirus Article from Fox News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing an article, let's parse several articles from Fox News. \n",
    "\n",
    "Similar to the way we parse several articles about coronavirus from The New York Times, we can set the date range, article type and section type to search several articles that meet our needs.\n",
    "\n",
    "However, the biggest difference between parsing several articles in The New York Times is that the url would not change when you manually set the conditions. Therefore, you have to write an `auto_click` function to help you `auto_click` on the dropdown checkboxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_click(driver, s, c, min_month_id, min_day_id, max_month_id, max_day_id, year_id):\n",
    "    '''\n",
    "    A helper fuction that pass in a webdriver object, section(s), content(c), \n",
    "    min_month_id, min_day_id, max_month_id, max_day_id, year_id \n",
    "    to specify the users condition when searching the articles on Fox News.\n",
    "    Return a webdriver object for further usage.\n",
    "    '''\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # --------------Section--------------\n",
    "    section = driver.find_element_by_css_selector(\"div.filter.section\")\n",
    "    section.click()\n",
    "    section.find_element_by_css_selector(\"ul.option>li>label>input[value=\\\"%s\\\"]\"%s).click()\n",
    "    section.click()\n",
    "    # --------------Content--------------\n",
    "    content = driver.find_element_by_css_selector(\"div.filter.content\")\n",
    "    content.click()\n",
    "    content.find_element_by_css_selector(\"ul.option>li>label>input[value=\\\"%s\\\"]\"%c).click()\n",
    "    content.click()\n",
    "    # --------------DateRange--------------\n",
    "    # -------Start-------\n",
    "    min_month = driver.find_element_by_css_selector(\"div.date.min div.sub.month\")\n",
    "    min_month.click()\n",
    "    min_month.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%min_month_id).click()\n",
    "\n",
    "    min_day = driver.find_element_by_css_selector(\"div.date.min div.sub.day\")\n",
    "    min_day.click()\n",
    "    min_day.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%min_day_id).click()\n",
    "\n",
    "    min_year = driver.find_element_by_css_selector(\"div.date.min div.sub.year\")\n",
    "    min_year.click()\n",
    "    min_year.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%year_id).click()\n",
    "    # --------End--------\n",
    "    max_month = driver.find_element_by_css_selector(\"div.date.max div.sub.month\")\n",
    "    max_month.click()\n",
    "    max_month.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%max_month_id).click()\n",
    "\n",
    "    max_day = driver.find_element_by_css_selector(\"div.date.max div.sub.day\")\n",
    "    max_day.click()\n",
    "    max_day.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%max_day_id).click()\n",
    "\n",
    "    max_year = driver.find_element_by_css_selector(\"div.date.max div.sub.year\")\n",
    "    max_year.click()\n",
    "    max_year.find_element_by_css_selector(\"ul.option>li[id=\\\"%s\\\"]\"%year_id).click()\n",
    "\n",
    "    search = driver.find_element_by_css_selector(\"div.search-form a\")\n",
    "    search.click()\n",
    "\n",
    "    time.sleep(5)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `auto_click` in `parse_several_pages_fn` to help you parse several pages in Fox News. Return two things:\n",
    "1. a list of tuple(include a dictionary and article word counts) in the order that they are present on the page\n",
    "2. the number of articles \n",
    "*Note: This time you might have to get the total article numbers specified on the top of the page.However, when return this value, please return len(list) to check if you retrieve all the articles.\n",
    "\n",
    "Note: You might need to make some changes of your `parse_page_fn` function since the article structure in Fox News is not unified as The New York Times. Try to handle all the exceptions!\n",
    "\n",
    "Set `Section=Health`, `Content=Article`, `Date Range from 2020/3/27~2020/4/4` for the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING parse_several_pages_fn: PASSED 11/11\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_several_pages_fn_test(parse_several_pages_fn):\n",
    "    articles, num_articles = parse_several_pages_fn(\"https://www.foxnews.com/search-results/search?q=coronavirus\")\n",
    "    \n",
    "    article_10, num_wc_10=articles[10]\n",
    "    article_check_fn(article_10)\n",
    "    test.equal(num_wc_10, 3014)\n",
    "    \n",
    "    article_38, num_wc_38=articles[38]\n",
    "    article_check_fn(article_38)\n",
    "    test.equal(num_wc_38, 2553)\n",
    "    \n",
    "    test.equal(num_articles, 93)\n",
    "\n",
    "@test    \n",
    "def parse_several_pages_fn(url, s='Health', c='Article', min_month_id='03', \\\n",
    "                           min_day_id='27', max_month_id='04', max_day_id='04', year_id='2020'):\n",
    "    \n",
    "    driver = retrieve_url_by_driver(url)\n",
    "    driver=auto_click(driver, s, c, min_month_id, min_day_id, max_month_id, max_day_id, year_id)   \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    p=int(soup.find(\"div\", class_=\"num-found\").find_all(\"span\")[2].get_text())\n",
    "\n",
    "    n=0\n",
    "    while n<p/10:\n",
    "        try:\n",
    "            time.sleep(3)\n",
    "            driver.find_element_by_css_selector(\"div.button.load-more> a >span\").click()\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "        n=n+1\n",
    "\n",
    "    time.sleep(10)        \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    ans=[]\n",
    "    n=1\n",
    "    for i in soup.select('h2.title a[href]'):\n",
    "#         print(i['href'])\n",
    "        ans.append(parse_page_fn(i['href']))\n",
    "#         print(n, \" Completed!\")\n",
    "        n=n+1\n",
    "    \n",
    "    return ans, len(ans)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}